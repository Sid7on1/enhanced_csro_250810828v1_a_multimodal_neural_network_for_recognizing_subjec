{
  "agent_id": "coder4",
  "task_id": "task_2",
  "files": [
    {
      "name": "requirements.txt",
      "purpose": "Python dependencies",
      "priority": "high"
    },
    {
      "name": "preprocessing.py",
      "purpose": "Image preprocessing utilities",
      "priority": "medium"
    }
  ],
  "project_info": {
    "project_name": "enhanced_cs.RO_2508.10828v1_A_Multimodal_Neural_Network_for_Recognizing_Subjec",
    "project_type": "computer_vision",
    "description": "Enhanced AI project based on cs.RO_2508.10828v1_A-Multimodal-Neural-Network-for-Recognizing-Subjec with content analysis. Detected project type: computer vision (confidence score: 10 matches).",
    "key_algorithms": [
      "Face",
      "Initial",
      "Each",
      "Attention",
      "Given",
      "Between",
      "Baseline",
      "Pretrained",
      "Learning",
      "All"
    ],
    "main_libraries": [
      "torch",
      "numpy",
      "pandas"
    ]
  },
  "paper_content": "PDF: cs.RO_2508.10828v1_A-Multimodal-Neural-Network-for-Recognizing-Subjec.pdf\nChunk: 1/1\n==================================================\n\n--- Page 1 ---\nA Multimodal Neural Network for Recognizing Subjective Self-Disclosure\nTowards Social Robots\nHenry Powell1,2\u2217, Guy Laban3,4,2\u2217, Emily S. Cross2,5\n1Amazon, Edinburgh, UK.\n2School of Psychology and Neuroscience, University of Glasgow, Glasgow, UK.\n3Department of Industrial Engineering and Management, Ben-Gurion University of the Negev, Beer Sheva, Israel.\n4Department of Computer Science and Technology, University of Cambridge, Cambridge, UK.\n5Social Brain Sciences Group, Department of Humanities, Social and Political Sciences, ETH Zurich, Zurich, Switzerland.\nAbstract \u2014 Subjective self-disclosure is an important feature\nof human social interaction. While much has been done in the\nsocial and behavioural literature to characterise the features\nand consequences of subjective self-disclosure, little work has\nbeen done thus far to develop computational systems that are\nable to accurately model it. Even less work has been done that\nattempts to model specifically how human interactants self-\ndisclose with robotic partners. It is becoming more pressing\nas we require social robots to work in conjunction with and\nestablish relationships with humans in various social settings. In\nthis paper, our aim is to develop a custom multimodal atten-\ntion network based on models from the emotion recognition\nliterature, training this model on a large self-collected self-\ndisclosure video corpus, and constructing a new loss function,\nthe scale preserving cross entropy loss, that improves upon\nboth classification and regression versions of this problem.\nOur results show that the best performing model, trained\nwith our novel loss function, achieves an F1 score of 0.83, an\nimprovement of 0.48 from the best baseline model. This result\nmakes significant headway in the aim of allowing social robots\nto pick up on an interaction partner\u2019s self-disclosures, an ability\nthat will be essential in social robots with social cognition.\nI. I NTRODUCTION\nSelf-disclosure is the sharing of one\u2019s thoughts, feelings,\nor personal information during a social interaction [1]. It is\nan import facet of human social behaviour and can contribute\nto many aspects of our lives. It can contribute to the extent to\nwhich we form bonds with one another - i.e. how intimate\nand important we consider our relationship with others to\nbe - as well as contributing significantly to our mental and\nphysical health [1]\u2013[3]. In this paper, we focus specifically\nonsubjective self-disclosure , which picks out the degree\nto which one perceives themselves to be sharing personal\ninformation with others. For example, it may be the case that\nsomeone shares some information which may, in general, not\n*Henry Powell and Guy Laban have contributed equally to this work and\nshare first authorship. Corresponding author: guy.laban@cl.cam.ac.uk\nThe authors gratefully acknowledge funding from the European Research\nCouncil (ERC) under the European Union\u2019s Horizon 2020 research and\ninnovation programme (Grant agreement 677270 to E.S.C.), the Leverhulme\nTrust (PLP-2018-152 to E.S.C.), and the European Union\u2019s Horizon 2020\nresearch and innovation programme under the Marie Sklodowska-Curie\nto ENTWINE, the European Training Network on Informal Care (grant\nagreement no. 814072 to E.S.C.). Our thanks also go to Hubert Ramsauer\nfor helpful discussions on the use of Hopfield networks.be perceived to be particularly personal or sensitive. How-\never, this information might well be meaningful to the person\ndisclosing it. Here the term subjective is supposed to clarify\nthat what is important in an act of self-disclosure is that\nthe person performing it believes themselves to be sharing\nmeaningful thoughts, feelings, or personal information.\nConsidering the importance of subjective self-disclosure\nin developing meaningful personal relationships, we argue\nthat sensitivity to these self-disclosures becomes a crucial\nattribute of social robots when establishing relationships\nwith them. Social robots are increasingly being studied in\nsocial contexts [4], with a focus on their affective roles\n[5], [6].Previous research has highlighted how individuals\ndevelop social relationships with these agents over time\n[7], [8], why users open up to them [9], [10], and the\ndiverse ways in which they can support human users [7],\n[8], [11]\u2013[13]. Therefore, these robotic agents, designed to\noperate among humans and communicate with them, must\nbe equipped to handle such disclosures appropriately [10].\nHowever, despite their potential, social robots currently lack\nthe nuanced cognitive capacity to infer such social infor-\nmation and understand subjective perceptions as effortlessly\nas humans do [14], which is critical when these robots take\nsocial and affective roles. Moreover, there is very little, if any,\nwork in the field of human\u2013robot interaction (HRI) that seeks\nto model the ability to detect and measure self-disclosure\nwith the aim bestowing a socially oriented artificial agent\nwith this ability. One such exception is a previous study\nfrom our group, where we found that a number of standard\ndeep learning architectures were able to perform well above\naverage on the task of ranking the degree of users\u2019 subjective\nself-disclosure in recorded interactions [15].\nIn this study, we aim to address this problem and to\nimprove previous results. We did this in a number of ways.\nFirstly, by developing a significantly larger data set that\nincluded an visual as well as an audio modality in order\nto capture markers of subjective self-disclosure that may be\npresent in how facial features evolve over time. Secondly,\nby developing a more sophisticated deep learning model that\nwas better suited to the task i.e. one that was developed using\ndomain knowledge of the problem and the data representa-\ntions we used as input to our model. Thirdly, to address thearXiv:2508.10828v1  [cs.RO]  14 Aug 2025\n\n--- Page 2 ---\nproblem of experimental framing that we experienced in that\nstudy, i.e. how to model data that was both categorical and\nscaled.\nA. The Current Paper\nThe remainder of the paper takes the following form: in\nSection II we detail the design, data collection and data\npre-processing for the experiment that we conducted in\norder to form the dataset used to perform our deep learning\nexperiments. Next, in Section III we outline which features\nwe extracted from the processed dataset and the means by\nwhich we extracted them. In Section IV, we describe the\narchitecture of our multi modal attention network in detail.\nWe then describe the experiments we conducted to produce\nbaselines to which we could compare the performance of\nthis model. Further, we detail the parameters of our ablation\nexperiment to test the effects of the loss functions, feature\nsets, and experimental framings that we used, and finally,\nthe specific details of the training implementation. Then, in\nSection V we present the results of the ablation study before\nfinally, in Section VI, discussing some areas for further\nimprovement to our approach and some issues with it.\nB. Our Contribution\nOur contributions to the field are as follows:\n1) We present the most extensive attempt to model sub-\njective self-disclosure in HRI so far,\n2) A multi modal attention based architecture designed\nspecifically for self-disclosure modelling from audio\nand video data,\n3) A novel loss function, the scale preserving cross en-\ntropy loss, that effectively deals with problems that fall\nbetween regression and classification and outperforms\nboth squared error and cross entropy approaches to\nself-disclosure modelling.\nII. D ATA SET AND DATA COLLECTION\nIn order to generate data for the models, a long-term\nmediated online experiment was conducted, as reported in\n[7]. We repeat that protocol here verbatim for consistency:\nA 2 (Discussion Theme: COVID-19 related vs. general)\nby 10 (chat sessions across time) between-groups repeated\nmeasures experimental design was followed. 39 Participants\nwere randomly assigned to one of the two discussion topic\ngroups, according to which they conversed with the robot\nPepper (SoftBank Robotics) via Zoom video chats about\ngeneral everyday topics (e.g., social relationships, work-life\nbalance, health and well-being). One group\u2019s conversation\ntopics were framed within the context of the COVID-19\npandemic (e.g., social relationships during the pandemic),\nwhereas the other group\u2019s conversation topics were similar,\nexcept that no explicit mention of the COVID-19 pandemic\nwas ever made. Participants were scheduled to interact with\nthe robot twice a week during prearranged times for five\nweeks, resulting in 10 interactions in total. Each interaction\nconsisted of the robot asking the participant 3 questions\n(x3 repetitions), starting with a generic question to buildrapport (e.g., how was your week/weekend), followed by\ntwo additional questions that corresponded to one of the\n10 randomly ordered topics (for the topics, questions, and\nexamples see [7]). The topic of each interaction was assigned\nrandomly, as was the order of the questions. After conversing\nwith Pepper via the zoom chat, participants filled a ques-\ntionnaire reporting for their perceptions of their subjective\ndisclosure via an adapted version of Jourad self-disclosure\nquestionnaire [1]. The zoom chats were recorded for analysis\npurposes. Each interaction with the robot lasted between\n5 to 10 minutes, and another 10-20 minutes were taken\nup completing questionnaires. The study followed rigorous\nethical standards and all study procedures were approved by\nthe research ethics committee of the School of Psychology\nand Neuroscience, University of Glasgow, UK.\nThis lead to 39\u00d710 = 390 interactions each comprising of\nat least 3 conversational segments that we were able to use to\ntrain our models. Once the dataset was collected the videos\nwere segmented by hand to isolate the sections that contained\nonly the participants\u2019 speech. Most videos contained three\nspeech segments comprised of the participants\u2019 answers\nto each of Pepper\u2019s questions. However, some participants\nfollowed up on Pepper\u2019s responses to their answers resulting\nin a number of additional speech segments that we were able\nto add to the corpus. Each of the segments was then labelled\nby an experimenter in accordance to the self-disclosure\nscore that each participant had assigned to their respective\ninteraction instances. This lead to a total of 1,248 speech\nand video segments that were used in our deep learning\nexperiments.\nIII. F EATURE EXTRACTION\nA. Visual Features\nWe extracted a number of visual feature types using a\ncombination of state-of-the art feature extraction models.\nFirst, we extracted frame-by-frame gaze and action unit\nfeatures using the OpenFace 2.2 library [16] (see Figure 2 for\nvisual example). To account for missing frames in each time\nseries that came about as a result of the OpenFace models not\nregistering the presence of a human face, we interpolated the\nmissing frames with the recorded data using spline interpola-\ntion. We then filtered and smoothed the resulting multivariate\ntime series with a Savitsky-Golay filter (using a sliding\nwindow of 11 frames and a polynomial order of 3). To test\nthe the affects of smoothing and filtering on the results we\ntreated smoothed/filtered and non-smoothed/filtered feature\nsets as separate in our initial experiments.\nNext, we extracted facial features using an InceptionV1\nResNet [17] [18] architecture pretrained on the VGGFace2\ndataset [19]. VGGFace2 consists of 3.31 million images of\ncelebrity faces organized into 9131 subject categories with\nlarge variances in pose, age, illumination, and ethnicity. The\nInceptionV1 ResNet that we used scored an accuracy of\n99.6% on this dataset. Pre-processing of the video frames\nin this case consisted of extracting a 160x160 pixel sub-\nregion of each frame that contained pixel and feature-wise\nnormalization of the subject\u2019s face (an example of the\n\n--- Page 3 ---\nFig. 1: Example output of the MTCNN used for facial feature extraction:\na 160x160 pixel normalised face image.\nFig. 2: OpenFace 2.2 processing facial action units, and gaze from an\ninput video.\nMTCNN output can be seen in Figure 1). This was done\nusing a pretrained multi-task cascaded convolutional neural\nnetwork [20] on each video frame. The pretrained ResNet\nproduces 512 facial features for each video frame. Similar to\nour approach with the the OpenFace features we interpolated\nand filtered the resulting time series to experiment with the\neffects that this would have the models\u2019 scores.\nB. Audio Features\nFor audio features we first produced a mel-filter cepstral\ncoefficient (MFCC) matrix for each video\u2019s audio modality.\nThis was done using PyTorch\u2019s audio feature extraction li-\nbrary using 256 mel-filter banks. This feature set was chosen\ndue MFCCs well established ability to capture significant\naudio features for human speech recognition tasks [21] [22]\n[23]. This was an extension from previous work on using\nlog-mel spectrograms to recognize subjective self-disclosures\n[15], where we found that spectrogram features were more\neffective at capturing significant self-disclosure related fea-\ntures from subjects\u2019 speech. In the case of this study, we\nfound that MFCC features produced better results at initial\ntesting and accordingly we went with MFCC features over\nthe log mel spectrogram alternative. We also experimented\nwith the effects of cepstral mean and variance normalization\nof MFCC features on our baseline models\u2019 performance\n(detailed in Section IV-A as this was a factor that would\nalso have to be taken into account when training our deep\nlearning models).\nSecond, we extracted audio features directly from each\nsound file\u2019s amplitude array using Facebook AIs wav2vec2.0\narchitecure [24]. Wav2vec2.0 uses a stack of convolutionalneural network based feature encoders and generates con-\ntextualised audio representations using a transformer model\n[25]. We used a wav2vec2.0 model pretrained on 960 hours\nof unlabelled audio data from the LibriSpeech dataset [26].\nTo get the feature sets for each wav file we took the outputs\nfrom the models 12 transformer layers which resulted in 12\ntx 768 feature matrices where the value t was determined\nby the number of frames in the audio file.\nIV. D EEPLEARNING EXPERIMENTS\nA. Support Vector Machine Baselines\nSince we were working with a novel dataset designed\nspecifically for our deep learning experiments we needed\nsome way of establishing a baseline that we were able to\ncompare our results to. Following [27] we used Gaussian\nkernel support vector machines (SVM) trained on our ex-\ntracted audio and visual features separately to establish such\na baseline. For each feature type, a vector representing the\nmean over all frames in each example was computed and\nthe SVMs were tasked with classifying the self-disclosure\nscore for each interaction. Each model was trained using 3\nfold cross validation and the average f1 score was used as a\nmeans to measure the overall performance of each model.\nThe results of these baseline experiments (illustrated in\nFigure 3) indicate that the facial features extracted using\nInceptionV1 pretrained on VGGFace2 were significantly the\nmost informative for the task while for the audio features,\nthe MFCC representation was the most informative. Overall\nvideo features were the most useful feature sets in discrmiiti-\nnating the self-disclosure score classes. The results also show\nthat the problem is a difficult one given that the best f1 score\nmeasured was only 0.36. One surprising result was that the\nword2vec2.0 features performed so poorly. We hypothesised\nthat, given the strong relative performance of the InceptionV1\nfeatures, that word2vec2.0 would also perform relatively well\ngiven that both models are pretrained on large amounts\nof task relevant data. One possible explanation of why\nword2vec2.0 features performed so poorly in the baseline test\nis with respect to how the mean vector for each frame was\ncomputed. The word2vec2.0 features for each frame were of\nfar higher dimension than both the MFCC features ( 12\u00d7tW\u00d7\n178vs.256\u00d7tM) and the visual feature set of the highest\ndimensionality (InceptionV1 features at tI\u00d7512)1. Thus\ncondensing the word2vec2.0 features across both the time\nand attention-head dimensions into a single 178 dimensional\nvector could have meant that too much information was lost\nleading to the feature dramatically losing its discriminative\nability with respect to the task.\nB. Multimodal Attention Network\nWe designed a multimodal attention network that pro-\ncesses the audio and visual features of each video in separate\nstreams and then combines these representations in a late\nfusion fashion before being classified by a linear neural\n1HeretW,tM, and tIrefer to the time dimension of the word2vec2.0\nfeatures, the MFCC features, and the InceptionV1 features respectively.\n\n--- Page 4 ---\nFig. 3: Gaussian SVM baseline F1 scores for individual smoothed/filtered\nand unsmoothed/unfiltered audio and visual feature sets. Standard deviation\nis represented by black error bars.\nnetwork layer. This approach was motivated by previous\nobservations communicated in [15] that concluded that \u2019off\nthe shelf\u2019 neural network architectures, i.e. ones that were\nnot designed specifically for the task at hand and used\nno pretraining, produced less than desirable results on the\naudio-only version of this task. We aimed to improve our\nprevious attempt (i.e., [15]) by: firstly, taking into account\nvideo recordings of the interactions. Secondly, designing a\ncustom neural network architecture that deals with audio\nand visual features separately before combining them into\none latent representation. Thirdly, using pretrained neural\nnetwork backbones in each feature processing stream and\nfinally, experimenting with feature fusion using principle\ncomponents analysis to prevent our results from being lim-\nited by being only able to use a single feature representation.\nThe design of this architecture is inspired by other deep\nlearning approaches that utilize attention mechanisms lever-\naged from deep convolutional neural networks for recogni-\ntion tasks involving visual and audio data captured from\nhuman subjects - specifically in emotion recognition and\nrelated tasks [28] [29]. Our approach is similar to that in [29]\nin that we use their convolutional architecture for each of\nthe attention mechanisms, although in our case we use only\nframe-wise attention in both the audio and visual streams.\nWe also use an InceptionV1 ResNet trained on VGGFace2\ninstead of the 3DResnet used in that study as it was more\nsuited to our problem and our baseline SVM experiment\nshowed convincingly that this feature representation was the\nmost informative for the task. As in [29] we compute the\nframe-wise attention (i.e. along the time dimension in each\ncase) for the audio and visual streams in the following way.\nWe adapt their formulation here for the sake of completeness\nand clarity with respect to how we have modified their\napproach. The full architecture is displayed in Figure 4\n1) Audio Temporal Attention Subnetwork: LetxA\nibe the\nithaudio feature matrix input. We first center crop xA\nito a\nfixed length lsuch thatl\ns\u2208Nfor some positive integer s\ngiving xA\u2032\ni. If the time dimension of xA\niis less than lthen we\npad the input on either side with zeros such that it\u2019s length is\nnow equal to l. We then split xA\u2032\niintossegments and stackthem on top of one another such that xA\u2032\ni\u2208Rs\u00d7l\ns\u00d7n. The\nmodel then receives a batch of size bof these tensors which\nis then fed through the model\u2019s audio stream.\nThe first step of the audio stream is to process each\nof the b\u00d7sfeature segments through a ResNet18 model\n[18] pretrained on the ImageNet dataset. This may sound\nsurprising given that we are using using an ImageNet trained\nmodel on MFCC audio representations (since ImageNet\ncontains no MFCC examples) but research has shown that\nusing such ResNets on MFCC features matrices dependably\nimproves model scores [30] and indeed we also found this to\nbe the case in our experiments. We then take the output FA\nj\nof the fifth convolutional stack of the pretrained ResNet18\nmodel and perform spatial average pooling over the feature\nmaps producing FA\u2032\nj(where jindexes over the feature matrix\nsegments). This downsamples the output of the ResNet from\nFA\nj\u2208Rs\u00d7h\u00d7w\u00d7ctoFA\u2032\nj\u2208Rs\u00d7cwhere sis the number\nof segments, handware the height and width of the\nfeature maps respectively, and cis the number of channels,\ncreating a 1\u00d7clength descriptor for each of the segments.\nThe goal is now to learn an s\u00d71length descriptor for\nthe audio feature matrix segments where the kthelement\nof the descriptor weights the kthsegment according to its\nimportance in classifying the input sample. This descriptor\nis learned using a convolutional stack that consists of a 1D\nconvolutional layer, a fully connected linear layer, and a\nReLU non-linearity such that:\nHA=WA\n1(WA\n2(FA\u2032\nj)T)T(1)\nWhere WA\n1andWA\n2ares\u00d7sand1\u00d7clearnable\nparameter matrices for the linear and convolutional layers\nrespectively. We then compute the activation of the audio\nattention subnetwork AAi.e. the s\u00d71length segment\ndescriptor as:\nAA=ReLU (HA) (2)\nThe output embedding for the audio stream, i.e. the\nrepresentation of which audio segments are most relevant\nto the classification of the input example to a particular self-\ndisclosure class, is computed via:\nEA=SX\nj=1FA\u2032\njAA(3)\n2) Visual Temporal Attention Subnetwork: The approach\nto achieve the audio embedding EVfor the visual features\nextracted from the videos follows precisely the same steps\nas the audio temporal attention algorithm. The principal\ndifferences in practice are that we use the InceptionV1\nResNet architecture trained on VGGFace2 that we used\nin our SVM baseline experiments instead of the ResNet18\nmodel.\nGiven the output embeddings EAandEVfor the au-\ndio and visual processing streams we then summarize the\n\n--- Page 5 ---\nFig. 4: Illustration of our multi-modal attention network. Segments of MFCC matrices (top) and face-cropped video frames (bot) are fed into two similar\nstreams. MFCC segments are fed through an ImageNet pretrained 2DResNet backbone before being average pooled, and cloned. One copy is then sent\nthrough the attention subnetwork before being multiplied to the other ResNet output copy. This representation is then average pooled once again producing\nthe final audio embedding. The same process occurs with the frame input except that the backbone is a InceptionV1 ResNet architecture pretrained on\nVGGFace2. The resulting audio and visual embeddings are then concatenated and fed through a linear classification layer. The network probabilities are\nthen used to compute the scale-preserving cross entropy loss by which the parameters of the network are optimised.\nfeatures using average pooling by computing the mean of\neach embedding vector along the time domain (i.e. across\nsegments) giving EA\u2032andEV\u2032. These are then concatenated\nbefore being fed to a linear layer containing 7 neurons\nrepresenting each of the self-disclosure score classes. This\nproduces output \u02c6y:\n\u02c6y=Softmax (WAVconcat (EA, EV)) (4)\nwhere WAVis a learnable parameter matrix related to the\nlinear output layer, concat() is the concatenation operation,\nand Softmax() is the softmax function that return normalized\nprobabilities over the seven self-disclosure score classes.\nC. Ablation Experiment Parameters\nIn our experiments we tested the influence of two different\nvisual feature sets, two experimental framings, and four\ndifferent loss functions to determine the best configuration\nfor the problem.\n1) Visual Feature Sets: First, we wanted to test the\nefficacy of just the facial features output by the InceptionV1\nResNet architecture pretrained on VGGFace2 as our SVM\nexperiments showed that these were likely to be the most\ninformative visual features for the task. Next we wanted to\ntest a combination of all visual features that we extracted\nas detailed in Section IV-A. To reduce the dimensionality of\nthis feature space we concatenated all of the visual features\ntogether after the visual input has been passed through the\nResNetV1 backbone in the visual stream and performed\nprincipal components analysis with parameters set such that\n99% of the variance in the data was explained by the\nresulting feature matrix. This resulted in a dimensionalitychange in this feature space from a 555 dimensional feature\nvector to a 67 dimensional feature vector for each video\nframe.\n2) Classification Vs. Regression: In [15] we found that\nthere was a nuance in the approach to classifying self-\ndisclosure scores. As we state in that study, participants rated\nthe degree of self-disclosure in their interactions on a likert\nscale between 1 and 7. This means that each score falls into\na discrete class meaning that one plausible way to frame\nthe problem is as an n-class classification problem. However,\nloss functions related to n-class classification problems often\ntreat incorrect guesses in the same manner i.e. there is no\nsense in which one guess can be numerically represented as\nbeing closer to a correct guess than any of the other possible\nguesses. The self-disclosure score data, however, is scaled\nin the sense that a model guess of 2 for ground truth self-\ndisclosure score of 1 should be treated as a better guess\nthan 6 or 7. In this light an argument could be made that the\nproblem is better represented as a regression problem. In [15]\nwe found that framing the problem in both ways produced\nsimilar results and as such no clear empirically informed\ndecision could be made about what approach worked best. In\nlight of this we decided to test the effects of both approaches\non our results.\n3) Loss Function: We wanted to study the effect of\nloss function on the problem. Standardly, regression based\nmethods minimize a mean-squared error loss in order to\noptimize the parameters of a given model. Since we had no\ngood reason to suspect that this particular problem required\nan alternative regression-based loss function we chose only\nto base our regression results on the mean squared error\nloss. For the classification version of the problem we chose a\n\n--- Page 6 ---\ncategorical cross-entropy loss function for our experiments.\nFor this loss, research has shown that label smoothing, a\ntechnique whereby standard \u2019hard\u2019 labels are modified by a\nsmoothing parameter \u03b1viayLS\nk=yk(1\u2212\u03b1) +\u03b1\nKwhere k\nindexes over the total number of classes (seven in the case of\nthis study), can drastically improve results [31]. As such we\nchose to include a cross entropy loss with label smoothing\nas part of ablation study. Last, we wanted to explore the\npossibility of designing a custom loss function that was able\nto strike a balance between the classification and regression\nversions of the task i.e. one that leveraged the fact that the\ndata was categorical while also preserving the notion that\ncertain guesses were better with respect to a ground truth\nlabel than others. Taking inspiration from [29] we designed\na custom cross entropy loss function that penalises guesses\nwith greater severity the further they are from the ground\ntruth label. For example, for an input sample with labelled\nself-disclosure score of 7 a guess of 1 will result in a higher\nloss than a guess of 2, a guess of 2 will result in a higher\nloss than a guess of 3, and so on. To do this we amended the\nstandard cross-entropy loss function which can be expressed\nas:\nLCE=\u22121\nNNX\ni=1CX\nc=11[c=yi]logpi,c (5)\nwhere N is the number of input samples, C is the number\nof classes, 1[c=yi]is an indicator variable that equals 1 when\nthe predicted class is the same as the ground truth class\nandpi,cis the probability that the ithsample belongs to\nthecthclass. We added a penalty term to 5 that formalizes\nthe idea that guesses at a greater distance from the ground\ntruth should be penalised more severely. This gives what we\nterm the scale preserving cross-entropy loss:\nLSPCE =\u22121\nNNX\ni=1(1 +\u03bb(|y\u2212\u02c6y|)\u00b5)CX\nc=11[c=yi]logpi,c\n(6)\nwhere \u03bband\u00b5are hyperparamters that change the degree\nto which a incorrect guess is penalised with respect to how\nfar away it is from the ground truth self-disclosure label.\nTaken together, the parameters of the ablation study lead\nto eight different training configurations for the multimodal\nattention network, the specifications of which are presented\nin Figure 5.\nD. Model Training\nRegression and classification models were trained over 100\nepochs, while the SPCE models where trained on 150 since\nwe found that they took longer to converge. All network\nversion we trained using the Adam optimizer [32], an initial\nlearning rate of 0.01, and mini-batch size of 35. Audio\nfeature inputs were cropped to length l= 128 and divided\nin to s= 4 segments. Visual input features were cropped\ntol= 210 frames and divided into s= 7 segments. Weprepared the training data as in [15] splitting the training and\ntesting datasets into an 80/20 split and used weighted random\nsampling to account for imbalanced classes. Each model was\ntrained five times and the average F1 score and standard\ndeviation over all five training instances were computed to\ngive a balanced assessment of the model\u2019s performance. We\nchose to validate the models using f1 scores so that our\nresults were directly comparable to those produced by our\nSVM experiments.\nV. R ESULTS\nThe results of our study are displayed in 5. We found\nthat all versions of the multimodal attention network scored\nsignificantly above the best SVM baseline. Interestingly,\ndeparting from [15], where regression and classification\nmodels performed about as well as each other, we found that\na classification framing (treating self-disclosure scores as dis-\ncrete classes) was significantly more effective at modelling\nthe problem than a regression framing (treating the scores as\nbeing derived from the continuous number line). In all cases\nwe found that, within each experimental framing, the features\nderived from principle components analysis outperformed\nmodels trained on just InceptionV1 facial features. This\nis perhaps unsurprising for two reasons. Firstly, because\nthis feature set was comprised of three times the number\nof features than the pure InceptionV1 feature set before\nit was condensed to its principal components. Secondly,\nbecause significantly reducing the number of features (from\n512 in the pure InceptionV1 case to 67 in the principal\ncomponents case) would mean that our model was less\nsusceptible to the curse of dimensionality i.e. that it would\nrequire much less data to effectively model that smaller set\nof features. Further, we found that label smoothing produced\nimprovements in results when compared to the non-label\nsmoothing variant of the cross entropy loss. Finally, we found\nthat our scale preserving cross-entropy loss outperformed all\nbut one version of the model (principal component features\nwith label smoothing cross-entropy loss) to which it equalled\nin performance.\nVI. D ISCUSSION AND CONCLUSION\nOverall we report significant increases on performance in\nthis task from previous attempts (e.g., [15]). We hypothesise\nthat this is due to a number of significant developments\nfrom that work. Firstly, we collected a much larger dataset\nmeaning that the models had more examples to learn from.\nSecond, in this case all interactions were recorded between\nthe same interaction dyad (i.e. between a human and a Pepper\nrobot). In [15], we collected interaction data between three\ndifferent dyads: human\u2013human, human\u2013embodied robot, and\nhuman\u2013voice agent. One reason that results may have been\nworse in that case is due to the possibility that vocal features\nparticular to each self-disclosure class may have been mod-\nulated by the kind of agent the participant was interacting\nwith. Since the interaction partner remained constant in our\nstudy, variability was reduced, which could have simplified\nthe learning task. Moreover, we used significantly more\n\n--- Page 7 ---\nFig. 5: F1 scores for our multimodal attention network trained on a\ndifferent combination of data input representations (principal components\nanalysis data (PCA), face features only (FF)) and loss functions (categorical\ncross entropy (CE), cross entropy with label smoothing (SE), mean squared\nerror (MSE), and our scale preserving cross entropy loss (SPCE)). We have\nalso colour coded the different experimental framings we used for the deep\nlearning experiments.\nadvanced models, taking advantage of the representational\npower of large deep neural networks trained on extensive\ndatasets. Further, our use of frame-wise attention mecha-\nnisms make use of deep learning techniques that have shown\nto be state-of-the art on video and language modelling tasks\nperhaps providing a straightforward upgrade of the \u2019off-the-\nshelf\u2019 models that were used in the previous study. Lastly,\nand perhaps most obviously, in this study we modelled two\ndifferent sensory modalities (audio and visual) as opposed\nto the single sensory modality that was considered in [15].\nIt may well be the case that the auditory domain holds\nless discriminative information than the visual domain for\nself-disclosure modelling and thus, the previous study was\nautomatically at a disadvantage in only considering the\nformer.\nOne surprising observation from our baseline experiments\nwas that the visual features were most effective at allowing\nthe SVM models to predict a particular subjective self-\ndisclosure score. While much of the literature on self-\ndisclosure is varied with respects to its definitions one\nthing that is generally agreed upon is that self-disclosure is\nprimarily a verbally communicated social phenomenon [33],\n[34]. In light of this it might be expected that the audio\nmodality would produce the best results. It may well be the\ncase that the way in which the audio features were averaged\ncaused some of the information to be lost. Unfortunately, a\nthorough investigation of why the visual features were the\nmost informative is outside of the scope of this paper.\nWhile this study shows significant improvement on pre-\nvious work done on modelling self-disclosure with neural\nnetworks, it remains to be seen whether the advances that\nwe detail here are significant enough for these models to\nbe implemented in social robots. There is a significant risk\nassociated with an incorrect self-disclosure scoring in a real\nworld setting [15]. Assuming that a person is sharing very\nlittle self-disclosure when in fact they believe themselves to\nbe sharing a significant amount could lead to that personfeeling as if they are being ignored or that the sensitive\ninformation that they are sharing is not worthy of the\nlistener\u2019s consideration [35]. Conversely, assigning a very\nhigh self-disclosure score in a situation where an interaction\npartner does not believe themselves to be sharing a significant\namount of personal information could cause undue levels of\nattention to be paid to a situation which is not important\n[36]. The issue described in both of these cases would be\nsignificantly confounded within the context of emotional\nwell being interventions, where the risks associated with not\npicking up on a user\u2019s self-disclosure related signals could\nbe very damaging. As such, considerably more work needs\nto be done before models like ours are considered for real\nworld application. There are at least two ways that steps\ncould be taken in this direction. Firstly, significantly more\ndata should be collected to improve the performance of the\nmodels. Secondly, a study should be carried out to asses the\ndifferences between model performance and the performance\non the same task by a trained professional. It is often the\ncase that the quality of a machine learning model and it\u2019s\nviability as a real world application is measure with respect\nto its ability to achieve \u2019human-like\u2019 performance. It makes\nsense that a model that is effective at recognizing the degree\nto which a person is disclosing personal information should\nbe able to do so at least as well as a trained professional\n(particularly if that model is to be implemented within the\ncontext of health care interventions).\nFurther, there are ways in which improvements on our\napproach might be made in the short term. Firstly, since\nwe found that performance on the task was improved when\nvisual features were combined using principal components\nanalysis, it\u2019s likely to also be the case that performance im-\nprovements could be achieved by combining audio features.\nIn particular, we did not experiment with ways to combine\noutputs from the transformer layers of wave2vec2.0 with the\nMFCC features. Additionally, more empirical work could be\ndone to ascertain the best way to combine feature sets in\nboth the audio and visual cases. For one such example, [27]\nused a denoising autoencoder to learn a compressed latent\nrepresentation of the concatenated input features. A future\nstudy should empirically test the hypothesis that such a latent\nrepresentation exists in a more effective input feature space\nthan the one produced by principal components analysis.\nFurther studies could also look into experimenting with other\nkinds of attention. In [29] the authors use channel-wise\nattention and spatial attention in the visual stream on top\nof the frame-wise attention that both of our methods share.\nOne development along these lines could be to implement\nan attention mechanism that produces a descriptor over the\nfeatures i.e. the columns of the input matrices. In this way the\nmodel would hold a representation of not only which frames\nof the input are important to its classification but also which\nfeatures are important. Lastly, the model could be altered to\nleverage 3D ResNets to produce higher dimensional features\nover the input video frames. This approach however would\nrequire a 3DResNet trained on a very large video dataset\nfocused on the modelling of human faces and, to our knowl-\n\n--- Page 8 ---\nedge, no such pretrained model is publicly available. Taking\nfrom the modelling literature on self-disclosure [37], show\nthat very good results on the task of (non-subjective) self-\ndisclosure modelling between two human interactants can be\nachieved multi-modally with the addition of lexical features.\nIn that study, the authors use a pretrained BERT language\nmodel [38] to extract features related to the words used in\neach utterance. A significant part of self-disclosure (at least\nin the human\u2013human case) is thought to be communicated\nverbally [33] [34]. This a future study could look at including\nthis modality in the HRI version of the task.\nWe believe that this study makes significant strides into the\nnew field of subjective self-disclosure modelling by showing\nconsiderable improvements over results of any previous\nstudies on the topic.\nREFERENCES\n[1] S. M. Jourard, Self-disclosure: An experimental analysis of the trans-\nparent self . Oxford, England: John Wiley, 1971.\n[2] H. Kreiner and Y . Levi-Belz, \u201cSelf-Disclosure Here and Now: Com-\nbining Retrospective Perceived Assessment With Dynamic Behavioral\nMeasures,\u201d Frontiers in Psychology , vol. 10, p. 558, 2019.\n[3] S. M. Jourard and P. Lasakow, \u201cSome factors in self-disclosure,\u201d The\nJournal of Abnormal and Social Psychology , vol. 56, no. 1, pp. 91\u201398,\n1958.\n[4] A. Henschel, G. Laban, and E. S. Cross, \u201cWhat Makes a Robot\nSocial? A Review of Social Robots from Science Fiction to a Home\nor Hospital Near You,\u201d Current Robotics Reports , no. 2, pp. 9\u201319,\n2021.\n[5] N. Churamani, S. Kalkan, and H. Gunes, \u201cContinual Learning for\nAffective Robotics: Why, What and How?,\u201d 29th IEEE International\nConference on Robot and Human Interactive Communication, RO-\nMAN 2020 , pp. 425\u2013431, 8 2020.\n[6] M. Spitale, M. Axelsson, S. Jeong, P. Tuttos\u0131, C. A. Stamatis,\nG. Laban, A. Lim, and H. Gunes, \u201cPast, Present, and Future: A\nSurvey of The Evolution of Affective Robotics For Well-being,\u201d IEEE\nTransactions in Affective Computing , 2025.\n[7] G. Laban, A. Kappas, V . Morrison, and E. S. Cross, \u201cBuilding Long-\nTerm Human\u2013Robot Relationships: Examining Disclosure, Percep-\ntion and Well-Being Across Time,\u201d International Journal of Social\nRobotics , vol. 16, no. 5, pp. 1\u201327, 2024.\n[8] G. Laban, V . Morrison, A. Kappas, and E. S. Cross, \u201cCoping with\nEmotional Distress via Self-Disclosure to Robots: An Intervention\nwith Caregivers,\u201d International Journal of Social Robotics , 2025.\n[9] G. Laban, A. Kappas, V . Morrison, and E. S. Cross, \u201cOpening Up\nto Social Robots: How Emotions Drive Self-Disclosure Behavior,\u201d\nin2023 32nd IEEE International Conference on Robot and Human\nInteractive Communication (RO-MAN) , (Busan, Republic of Korea),\npp. 1697\u20131704, IEEE, 8 2023.\n[10] G. Laban and E. S. Cross, \u201cSharing our Emotions with Robots: Why\ndo we do it and how does it make us feel?,\u201d IEEE Transactions on\nAffective Computing , 2024.\n[11] N. L. Robinson, T. V . Cottier, and D. J. Kavanagh, \u201cPsychosocial\nHealth Interventions by Social Robots: Systematic Review of Random-\nized Controlled Trials,\u201d J Med Internet Res , vol. 21, no. 5, pp. 1\u201320,\n2019.\n[12] G. Laban, V . Morrison, and E. Cross, \u201cSocial robots for health\npsychology: A new frontier for improving human health and well-\nbeing,\u201d European Health Psychologist , vol. 23, pp. 1095\u20131102, 2 2024.\n[13] G. Laban, S. Chiang, and H. Gunes, \u201cWhat People Share With a Robot\nWhen Feeling Lonely and Stressed and How It Helps Over Time,\u201d in\n2025 34th IEEE International Conference on Robot and Human In-\nteractive Communication (ROMAN) , (Eindhoven, Netherlands), IEEE,\n2025.\n[14] E. S. Cross, R. Hortensius, and A. Wykowska, \u201cFrom social brains to\nsocial robots: applying neurocognitive insights to human-robot inter-\naction,\u201d Philosophical Transactions of the Royal Society B: Biological\nSciences , vol. 374, no. 1771, p. 20180024, 2019.[15] H. Powell, G. Laban, J.-N. George, and E. S. Cross, \u201cIs deep learning a\nvalid approach for inferring subjective self-disclosure in human-robot\ninteractions?,\u201d in Proceedings of the 2022 ACM/IEEE International\nConference on Human-Robot Interaction , Hri \u201922, p. 991\u2013996, IEEE\nPress, 2022.\n[16] T. Baltrusaitis, A. Zadeh, Y . C. Lim, and L.-P. Morency, \u201cOpenface\n2.0: Facial behavior analysis toolkit,\u201d in 2018 13th IEEE international\nconference on automatic face & gesture recognition (FG 2018) ,\npp. 59\u201366, Ieee, 2018.\n[17] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed, D. Anguelov,\nD. Erhan, V . Vanhoucke, and A. Rabinovich, \u201cGoing deeper with\nconvolutions,\u201d in Proceedings of the IEEE conference on computer\nvision and pattern recognition , pp. 1\u20139, 2015.\n[18] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for image\nrecognition,\u201d in Proceedings of the IEEE conference on computer\nvision and pattern recognition , pp. 770\u2013778, 06 2016.\n[19] Q. Cao, L. Shen, W. Xie, O. M. Parkhi, and A. Zisserman, \u201cVg-\ngface2: A dataset for recognising faces across pose and age,\u201d CoRR ,\nvol. abs/1710.08092, 2017.\n[20] K. Zhang, Z. Zhang, Z. Li, and Y . Qiao, \u201cJoint face detection and\nalignment using multitask cascaded convolutional networks,\u201d IEEE\nSignal Processing Letters , vol. 23, no. 10, pp. 1499\u20131503, 2016.\n[21] N. Yang, N. Dey, R. S. Sherratt, and F. Shi, \u201cRecognize basic\nemotional statesin speech by machine learning techniques using mel-\nfrequency cepstral coefficient features,\u201d Journal of Intelligent & Fuzzy\nSystems , vol. 39, no. 2, pp. 1925\u20131936, 2020.\n[22] M. D. Pawar and R. D. Kokate, \u201cConvolution neural network based\nautomatic speech emotion recognition using mel-frequency cepstrum\ncoefficients,\u201d Multimedia Tools and Applications , vol. 80, no. 10,\npp. 15563\u201315587, 2021.\n[23] U. Kumaran, S. Radha Rammohan, S. M. Nagarajan, and A. Prathik,\n\u201cFusion of mel and gammatone frequency cepstral coefficients for\nspeech emotion recognition using deep c-rnn,\u201d International Journal\nof Speech Technology , vol. 24, no. 2, pp. 303\u2013314, 2021.\n[24] A. Baevski, Y . Zhou, A. Mohamed, and M. Auli, \u201cwav2vec 2.0: A\nframework for self-supervised learning of speech representations,\u201d in\nAdvances in Neural Information Processing Systems (H. Larochelle,\nM. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, eds.), vol. 33,\npp. 12449\u201312460, Curran Associates, Inc., 2020.\n[25] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d\ninProceedings of the 31st International Conference on Neural\nInformation Processing Systems , Nips\u201917, (Red Hook, NY , USA),\np. 6000\u20136010, Curran Associates Inc., 2017.\n[26] V . Panayotov, G. Chen, D. Povey, and S. Khudanpur, \u201cLibrispeech:\nAn asr corpus based on public domain audio books,\u201d in 2015 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing\n(ICASSP) , pp. 5206\u20135210, 2015.\n[27] W. Lin, I. Orton, Q. Li, G. Pavarini, and M. Mahmoud, \u201cLooking at\nthe body: Automatic analysis of body gestures and self-adaptors in\npsychological distress,\u201d IEEE Transactions on Affective Computing ,\n2021.\n[28] Z. Zhao, Q. Li, Z. Zhang, N. Cummins, H. Wang, J. Tao, and B. W.\nSchuller, \u201cCombining a parallel 2d cnn with a self-attention dilated\nresidual network for ctc-based discrete speech emotion recognition,\u201d\nNeural Networks , vol. 141, pp. 52\u201360, 2021.\n[29] S. Zhao, Y . Ma, Y . Gu, J. Yang, T. Xing, P. Xu, R. Hu, H. Chai, and\nK. Keutzer, \u201cAn end-to-end visual-audio attention network for emotion\nrecognition in user-generated videos,\u201d CoRR , vol. abs/2003.00832,\n2020.\n[30] K. Palanisamy, D. Singhania, and A. Yao, \u201cRethinking cnn models for\naudio classification,\u201d arXiv preprint arXiv:2007.11154 , 2020.\n[31] L. Yuan, F. E. Tay, G. Li, T. Wang, and J. Feng, \u201cRevisiting knowledge\ndistillation via label smoothing regularization,\u201d in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition ,\npp. 3903\u20133911, 2020.\n[32] D. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d\nInternational Conference on Learning Representations , 12 2014.\n[33] P. C. Cozby, \u201cSelf-disclosure: a literature review.,\u201d Psychological\nbulletin , vol. 79, no. 2, p. 73, 1973.\n[34] J. Omarzu, \u201cA Disclosure Decision Model: Determining How and\nWhen Individuals Will Self-Disclose,\u201d Pers Soc Psychol Rev , vol. 4,\nno. 2, pp. 174\u2013185, 2000.\n[35] N. L. Collins and L. C. Miller, \u201cSelf-disclosure and liking: a meta-\nanalytic review,\u201d Psychological bulletin , vol. 116, pp. 457\u2013475, 1994.\n\n--- Page 9 ---\n[36] Z. A. Reese and K. Orrach, \u201cReciprocal self-disclosure: Although\nrespondents are reluctant to steal the spotlight, self-disclosers feel val-\nidated, understood, and cared for when respondents share comparable\nexperiences,\u201d Journal of Social and Personal Relationships , vol. 40,\npp. 3485\u20133514, 5 2023.\n[37] M. Soleymani, K. Stefanov, S.-H. Kang, J. Ondras, and J. Gratch,\n\u201cMultimodal analysis and estimation of intimate self-disclosure,\u201d in\n2019 International Conference on Multimodal Interaction , pp. 59\u201368,2019.\n[38] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBERT: Pre-\ntraining of deep bidirectional transformers for language understand-\ning,\u201d in Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short Papers) , (Min-\nneapolis, Minnesota), pp. 4171\u20134186, Association for Computational\nLinguistics, June 2019.",
  "project_dir": "artifacts/projects/enhanced_cs.RO_2508.10828v1_A_Multimodal_Neural_Network_for_Recognizing_Subjec",
  "communication_dir": "artifacts/projects/enhanced_cs.RO_2508.10828v1_A_Multimodal_Neural_Network_for_Recognizing_Subjec/.agent_comm",
  "assigned_at": "2025-08-16T20:52:25.034683",
  "status": "assigned"
}